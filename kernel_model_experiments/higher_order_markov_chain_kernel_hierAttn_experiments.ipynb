{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments w Kernel Mechanisms for External Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model a \"language modeling\" task of predicting the next \"word\" in terms of a higher-order Markov models.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{M} &= \\{(x^{(t)}, y^{(t)})\\}_{t \\in [n_m]}\\\\\n",
    "x^{(t)} &= (x_1^{(t)}, \\ldots, x_l^{(t)})\\\\\n",
    "y^{(t)} &= (y_1^{(t)}, \\ldots, y_l^{(t)}) = (x_2^{(t)}, \\ldots, x_{l+1}^{(t)})\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $x_i^{(t)}$ come from a hidden markov model.\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_i^{(t)} &\\sim P(\\cdot | x_{i-1}^{(t)}, \\ldots, x_{i-m}^{(t)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import itertools\n",
    "import sys;sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 16\n",
    "m = 2\n",
    "\n",
    "state_tuples = list(itertools.product(range(n_states), repeat=m))\n",
    "len(state_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict = dict()\n",
    "\n",
    "for state_tuple in state_tuples:\n",
    "    next_state = np.random.randint(n_states)\n",
    "    p = np.zeros(n_states)\n",
    "    p[next_state] = 1\n",
    "\n",
    "    transition_dict[state_tuple] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_markov_chain(transition_dict, prefix, num_steps):\n",
    "    states_seq = prefix\n",
    "    m = len(list(transition_dict.keys())[0])\n",
    "\n",
    "    for i in range(num_steps-m):\n",
    "        probabilities = transition_dict[tuple(states_seq[-m:])]\n",
    "        next_state = np.random.choice(range(n_states), p=probabilities)\n",
    "        states_seq.append(next_state)\n",
    "    return states_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 20 # length of input sequences\n",
    "mem_seq_len = 4*m # length of each memory sequence\n",
    "mem_size = 16 # number of memory sequences\n",
    "\n",
    "n_train = 600\n",
    "n_test = 400\n",
    "\n",
    "def sample_seqs_dataset(n, input_seq_len):\n",
    "    seqs = np.array([sample_markov_chain(\n",
    "        transition_dict, prefix=list(state_tuples[np.random.choice(len(state_tuples))]), num_steps=input_seq_len+1) for _ in range(n)])\n",
    "\n",
    "    seqs_x = seqs[:, :-1]\n",
    "    seqs_y = seqs[:, 1:]\n",
    "\n",
    "    return seqs_x, seqs_y\n",
    "\n",
    "\n",
    "def sample_mem_dataset(mem_seq_len, mem_size):\n",
    "    mem_seqs = np.array([sample_markov_chain(\n",
    "        transition_dict, prefix=list(state_tuples[np.random.choice(len(state_tuples))]), num_steps=mem_seq_len+1) \n",
    "         for _ in range(mem_size)])\n",
    "    # mem_seqs = np.repeat(np.expand_dims(mem_seqs, axis=0), axis=0, repeats=n)\n",
    "    mem_seqs = mem_seqs\n",
    "\n",
    "    mem_seqs_x = mem_seqs[:, :-1]\n",
    "    mem_seqs_y = mem_seqs[:, 1:]\n",
    "\n",
    "    return mem_seqs_x, mem_seqs_y\n",
    "\n",
    "seqs_x_train, seqs_y_train = sample_seqs_dataset(n_train, input_seq_len)\n",
    "seqs_x_test, seqs_y_test = sample_seqs_dataset(n_test, input_seq_len)\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: manually add some patterns that appears in training and testing set to memory buffer \n",
    "\n",
    "mem_seqs_x, mem_seqs_y = sample_mem_dataset(mem_seq_len, mem_size)\n",
    "\n",
    "\n",
    "# sample memory buffers randomly\n",
    "mem_seqs_x_train, mem_seqs_y_train = zip(*[sample_mem_dataset(mem_seq_len, mem_size) for _ in range(n_train)])\n",
    "mem_seqs_x_test, mem_seqs_y_test = zip(*[sample_mem_dataset(mem_seq_len, mem_size) for _ in range(n_test)])\n",
    "\n",
    "mem_seqs_x_train, mem_seqs_y_train = np.array(mem_seqs_x_train), np.array(mem_seqs_y_train)\n",
    "mem_seqs_x_test, mem_seqs_y_test = np.array(mem_seqs_x_test), np.array(mem_seqs_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'seqs_x_train.shape: {seqs_x_train.shape}; seqs_y_train.shape: {seqs_y_train.shape}') # shape [batch_size, seq_len]\n",
    "print(f'mem_seqs_x_train.shape: {mem_seqs_x_train.shape}; mem_seqs_y.shape: {mem_seqs_y_train.shape}') # shape [batch_size, num_mem_seqs, mem_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_kwargs = dict(epochs=30)\n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm\n",
    "def create_callbacks():\n",
    "    return [TqdmCallback(tqdm_class=tqdm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Kernel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_train, mem_seqs_x_train), seqs_y_train)).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_test, mem_seqs_x_test), seqs_y_test)).batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernel_memory_transformer import KernelMemoryTransformer\n",
    "vocab_size = len(state_tuples)\n",
    "embedding_dim = 64\n",
    "memory_model1 = KernelMemoryTransformer(\n",
    "    vocab_size, embedding_dim=embedding_dim,\n",
    "    key_dim=embedding_dim//4, n_heads=4, ff_sizes = [vocab_size//2,], name='kernel_memory_model')\n",
    "\n",
    "memory_model1([seqs_x_train[:64], mem_seqs_x_train[:64]]); # build model\n",
    "create_opt = lambda: tf.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "memory_model1.compile(optimizer=create_opt(), loss=loss, metrics=['accuracy'])\n",
    "memory_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = memory_model1.fit(train_ds, verbose=0, callbacks=create_callbacks(), **fit_kwargs)\n",
    "utils.plot_history(history1, plot_attrs=('loss', 'accuracy'), val=False, figsize=(12,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_model1.evaluate(train_ds)\n",
    "memory_model1.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model1.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: Hierarchical Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_attention import MultiHeadHierarchicalAttention\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_train, mem_seqs_x_train, mem_seqs_y_train), seqs_y_train)).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_test, mem_seqs_x_test, mem_seqs_y_test), seqs_y_test)).batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import AddPositionalEmbedding, create_positional_encoding, MemoryAddPositionalEmbedding\n",
    "from transformer_modules import GlobalSelfAttention, CausalSelfAttention\n",
    "\n",
    "class HierarchicalAttnMemoryModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, key_dim, n_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedder = layers.Embedding(vocab_size, embedding_dim, name='embedder')\n",
    "        self.pos_embedder_adder  = AddPositionalEmbedding(name='pos_embedder_adder')\n",
    "        self.memory_pos_embedder_adder = MemoryAddPositionalEmbedding(name='mem_pos_embedder_adder')\n",
    "\n",
    "        # different attention layers for input and memories\n",
    "        # self.self_attention_layer = layers.MultiHeadAttention(num_heads=n_heads, key_dim=key_dim, name='self_attn')\n",
    "        # self.memory_self_attention_layer = layers.MultiHeadAttention(num_heads=n_heads, key_dim=key_dim, name='mem_self_attn')\n",
    "        self.self_attention_layer = CausalSelfAttention(num_heads=n_heads, key_dim=key_dim, name='self_attn')\n",
    "        self.memory_self_attention_layer = GlobalSelfAttention(num_heads=n_heads, key_dim=key_dim, name='mem_self_attn')\n",
    "\n",
    "        # same attention layers for input and memories\n",
    "        # self.self_attention_layer = layers.MultiHeadAttention(num_heads=1, key_dim=key_dim, value_dim=embedding_dim, name='self_attn')\n",
    "        # self.memory_self_attention_layer = self.self_attention_layer\n",
    "\n",
    "        self.hierarchical_mem_attention = MultiHeadHierarchicalAttention(\n",
    "            key_dim, value_dim=embedding_dim, n_heads=n_heads,\n",
    "            attn_scale_factor_per_seq=1, attn_scale_factor_over_seqs=1,\n",
    "            dense_kwargs=dict(use_bias=False), name='mem_attn')\n",
    "\n",
    "        self.output_dense = layers.Dense(vocab_size, name='output')\n",
    "\n",
    "    def self_attention(self, seq):\n",
    "        # return self.self_attention_layer(seq, seq, use_causal_mask=True)\n",
    "        return self.self_attention_layer(seq)\n",
    "\n",
    "    def memory_self_attention(self, mem_seqs):\n",
    "        # lambda_fn = lambda x: self.memory_self_attention_layer(x, x, use_causal_mask=True)\n",
    "        lambda_fn = lambda x: self.memory_self_attention_layer(x)\n",
    "        return tf.map_fn(lambda_fn, mem_seqs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_input, memory_x_input, memory_y_input = inputs\n",
    "\n",
    "        embedded_input_seqs = self.embedder(seq_input)\n",
    "        embedded_mem_x_seqs = self.embedder(memory_x_input)\n",
    "        embedded_mem_y_seqs = self.embedder(memory_y_input)\n",
    "\n",
    "        embedded_input_seqs = self.pos_embedder_adder(embedded_input_seqs)\n",
    "        embedded_mem_x_seqs = self.memory_pos_embedder_adder(embedded_mem_x_seqs)\n",
    "        embedded_mem_y_seqs = self.memory_pos_embedder_adder(embedded_mem_y_seqs)\n",
    "\n",
    "        embedded_input_seqs = self.self_attention(embedded_input_seqs)\n",
    "        embedded_mem_x_seqs = self.memory_self_attention(embedded_mem_x_seqs)\n",
    "\n",
    "        retrieved_mems = self.hierarchical_mem_attention([embedded_input_seqs, embedded_mem_x_seqs, embedded_mem_y_seqs])\n",
    "\n",
    "        logits = self.output_dense(retrieved_mems)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(state_tuples)\n",
    "embedding_dim = 64\n",
    "\n",
    "memory_model2 = HierarchicalAttnMemoryModel(\n",
    "    vocab_size, embedding_dim=embedding_dim,\n",
    "    key_dim=embedding_dim//3, n_heads=3, name='memory_model')\n",
    "\n",
    "memory_model2([seqs_x_train, mem_seqs_x_train, mem_seqs_y_train]); # build model\n",
    "\n",
    "create_opt = lambda: tf.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "memory_model2.compile(optimizer=create_opt(), loss=loss, metrics=['accuracy'])\n",
    "memory_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = memory_model2.fit(train_ds, verbose=0, callbacks=create_callbacks(), **fit_kwargs)\n",
    "utils.plot_history(history2, plot_attrs=('loss', 'accuracy'), val=False, figsize=(12,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_model2.evaluate(train_ds)\n",
    "memory_model2.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model2.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: kernel model, use mem_seq_y as value in cross attention (only embedding, no self-attention on mem_seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from transformer_modules import AddPositionalEmbedding, MemoryAddPositionalEmbedding\n",
    "from attention import GlobalSelfAttention, CausalSelfAttention, CrossAttention\n",
    "\n",
    "class KernelMemoryTransformer_cross_y(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, key_dim, n_heads, ff_sizes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.alpha_it = None\n",
    "        self.embedder = layers.Embedding(vocab_size, embedding_dim, name='embedder')\n",
    "        self.pos_embedder_adder  = AddPositionalEmbedding(name='pos_embedder_adder')\n",
    "        self.memory_pos_embedder_adder = MemoryAddPositionalEmbedding(name='mem_pos_embedder_adder')\n",
    "\n",
    "        # different self-attention layers for input and memories for encoding sequences\n",
    "        self.input_self_attention = CausalSelfAttention(num_heads=n_heads, key_dim=key_dim, value_dim=embedding_dim//n_heads, name='self_attn')\n",
    "        self.memory_self_attention_layer = GlobalSelfAttention(num_heads=n_heads, key_dim=key_dim, value_dim=embedding_dim//n_heads, name='mem_self_attn')\n",
    "\n",
    "        # create cross attention layer\n",
    "        self.cross_attention_layer = layers.MultiHeadAttention(num_heads=n_heads, key_dim=key_dim, value_dim=embedding_dim//n_heads, name='cross_attn')\n",
    "\n",
    "        self.feedforward = tf.keras.Sequential([layers.Dense(z, activation='relu') for z in ff_sizes], name='feedforward')\n",
    "        self.output_dense = layers.Dense(vocab_size, activation='softmax', name='output')\n",
    "\n",
    "    def memory_self_attention(self, mem_seqs):\n",
    "        '''apply self-attention to each memory sequence'''\n",
    "        lambda_fn = lambda x: self.memory_self_attention_layer(x)\n",
    "        return tf.map_fn(lambda_fn, mem_seqs)\n",
    "\n",
    "    def get_cross_probs(self, embedded_input_seqs, embedded_mem_x_seqs, embedded_mem_y_seqs):\n",
    "        '''compute cross attention probabilities prob(x_{i+1} = . | x_{1:i}, x^{(t)}) for each memory'''\n",
    "\n",
    "        def get_memory_cross_probs(E_xt, E_yt):\n",
    "            cross_attention_output = self.cross_attention_layer(\n",
    "                query=embedded_input_seqs,\n",
    "                key=E_xt,\n",
    "                value=E_yt)\n",
    "            return cross_attention_output # shape: (batch_size, seq_len, mem_len)\n",
    "        \n",
    "        #print(get_memory_cross_probs(embedded_mem_x_seqs[:,0,:,:], embedded_mem_y_seqs[:,0,:,:]))\n",
    "        #compute cross-attention for each memory\n",
    "        cross_probs = tf.map_fn(lambda xy: get_memory_cross_probs(xy[0], xy[1]), (tf.transpose(embedded_mem_x_seqs, perm=[1, 0, 2, 3]), tf.transpose(embedded_mem_y_seqs, perm=[1, 0, 2, 3])), dtype=tf.float32)\n",
    "        cross_probs = tf.transpose(cross_probs, perm=[1, 0, 2, 3]) # shape: (batch_size, n_mem, mem_length, embedding_dim)\n",
    "\n",
    "        # compute cross-probs from cross-attention via feedforward network\n",
    "        cross_probs = self.feedforward(cross_probs)\n",
    "        cross_probs = self.output_dense(cross_probs) # shape: (batch_size, n_mems, mem_length, vocab_size)\n",
    "\n",
    "        return cross_probs\n",
    "\n",
    "    def get_observed_cross_probs(self, cross_probs, seq_input):\n",
    "        '''given the cross_probs tensor, extract the components coresponding to the observed sequence'''\n",
    "\n",
    "        cross_probs_observed = tf.transpose(cross_probs, perm=(1,0,2,3))\n",
    "        cross_probs_observed = tf.map_fn(lambda x: tf.gather(x, indices=seq_input, axis=-1, batch_dims=2), cross_probs_observed)\n",
    "        cross_probs_observed = tf.transpose(cross_probs_observed, perm=(1,0,2))\n",
    "        return cross_probs_observed\n",
    "\n",
    "    def compute_mem_attention_scores(self, cross_probs_observed):\n",
    "        '''compute memory-level attention scores, alpha_it (i is position in input sequence, t is memory index)'''\n",
    "\n",
    "        log_cross_probs_observed = tf.math.log(cross_probs_observed)\n",
    "        w_it = tf.math.cumsum(log_cross_probs_observed, axis=-1, exclusive=True)\n",
    "        alpha_it = tf.nn.softmax(w_it, axis=1) # shape [batch_size, num_mems, in_seq_length]\n",
    "        return alpha_it\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_input, memory_input_x, memory_input_y = inputs\n",
    "\n",
    "        # embed input sequences and memory sequences\n",
    "        embedded_input_seqs = self.embedder(seq_input)\n",
    "        embedded_mem_x_seqs = self.embedder(memory_input_x)\n",
    "        embedded_mem_y_seqs = self.embedder(memory_input_y)\n",
    "\n",
    "        # add positional embeddings\n",
    "        embedded_input_seqs = self.pos_embedder_adder(embedded_input_seqs)\n",
    "        embedded_mem_x_seqs = self.memory_pos_embedder_adder(embedded_mem_x_seqs)\n",
    "        embedded_mem_y_seqs = self.memory_pos_embedder_adder(embedded_mem_y_seqs)\n",
    "\n",
    "        # encode via self-attention\n",
    "        embedded_input_seqs = self.input_self_attention(embedded_input_seqs) # shape: (batch_size, input_length, embedding_dim)\n",
    "        embedded_mem_x_seqs = self.memory_self_attention(embedded_mem_x_seqs) # shape: (batch_size, num_memories, memory_length, embedding_dim)\n",
    "\n",
    "        # compute prob(x_{i+1} = . | x_{1:i}, x^{(t)}) for each memory\n",
    "        cross_probs = self.get_cross_probs(embedded_input_seqs, embedded_mem_x_seqs, embedded_mem_y_seqs)\n",
    "\n",
    "        # compute prob(x_{i+1} | x_{1:i}, x^{(t)}) for each memory\n",
    "        cross_probs_observed = self.get_observed_cross_probs(cross_probs, seq_input)\n",
    "\n",
    "        # compute alpha_it = softmax(w_it) where w_it = sum_{j=1}^{i-1} log prob(x_{j+1} | x_{1:j}, x^{(t)})\n",
    "        alpha_it = self.compute_mem_attention_scores(cross_probs_observed)\n",
    "        self.alpha_it = alpha_it\n",
    "        \n",
    "        # compute prob(x_{i+1} | x_{1:i}, {x^{(t)}}_t) = \\sum_t alpha_it * prob(x_{i+1} = . | x_{1:i}, x^{(t)})\n",
    "        output = tf.reduce_sum(tf.multiply(tf.expand_dims(alpha_it, axis=-1), cross_probs), axis=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_train, mem_seqs_x_train, mem_seqs_y_train), seqs_y_train)).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_test, mem_seqs_x_test, mem_seqs_y_test), seqs_y_test)).batch(64)\n",
    "\n",
    "\n",
    "memory_model3 = KernelMemoryTransformer_cross_y(\n",
    "    vocab_size, embedding_dim=embedding_dim,\n",
    "    key_dim=embedding_dim//4, n_heads=4, ff_sizes = [vocab_size//2,], name='kernel_memory_model_cross_y')\n",
    "\n",
    "memory_model3([seqs_x_train[:64], mem_seqs_x_train[:64], mem_seqs_y_train[:64]]); # build model\n",
    "create_opt = lambda: tf.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "memory_model3.compile(optimizer=create_opt(), loss=loss, metrics=['accuracy'])\n",
    "memory_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = memory_model3.fit(train_ds, verbose=0, callbacks=create_callbacks(), **fit_kwargs)\n",
    "utils.plot_history(history3, plot_attrs=('loss', 'accuracy'), val=False, figsize=(12,4));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_model3.evaluate(train_ds)\n",
    "memory_model3.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model3.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for analyzing attention patterns in hierarchical attention model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "def plot_per_seq_attention(attn_, mem_seqs_x, train_seqs_x):\n",
    "    num_mem_seqs = attn_.shape[0]\n",
    "    fig = plt.figure(figsize=(num_mem_seqs*2,3))\n",
    "    gs = gridspec.GridSpec(1, num_mem_seqs+1, width_ratios=[0.5]*num_mem_seqs+[0.1])\n",
    "    axs = [fig.add_subplot(gs[i]) for i in range(num_mem_seqs)]\n",
    "    cax = fig.add_subplot(gs[-1])\n",
    "    im = None\n",
    "    for t, ax in enumerate(axs):\n",
    "        im = ax.imshow(attn_[t], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_xticks(range(len(mem_seqs_x[t])))\n",
    "        ax.set_yticks(range(len(train_seqs_x)))\n",
    "        ax.set_xticklabels(mem_seqs_x[t]);\n",
    "        ax.set_yticklabels(train_seqs_x);\n",
    "    fig.colorbar(im, cax=cax)\n",
    "    axs[0].set_ylabel('input sequence')\n",
    "    fig.supxlabel('memory sequences', y=0.)\n",
    "    fig.suptitle('attention within each memory sequence')\n",
    "    return fig\n",
    "\n",
    "def plot_seq_attention(attn_, mem_seqs_x, train_seqs_x, **figkwargs):\n",
    "    fig, ax = plt.subplots(**figkwargs)\n",
    "    cmap = 'gray' if attn_.shape[-1] == 1 else None\n",
    "    im = ax.imshow(attn_, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_yticks(range(len(mem_seqs_x)))\n",
    "    ax.set_xticks(range(len(train_seqs_x)))\n",
    "    ax.set_yticklabels([tuple(x) for x in mem_seqs_x]);\n",
    "    ax.set_xticklabels(train_seqs_x);\n",
    "\n",
    "    ax.set_xlabel('input sequence')\n",
    "    ax.set_ylabel('memory sequences')\n",
    "    ax.set_title('attention over sequences')\n",
    "\n",
    "    fig.colorbar(im, ax=ax);\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_attention_hierarchical(sample_input_index, seqs_x_train, seqs_y_train, mem_seqs_x_train, mem_seqs_y_train, memory_model):\n",
    "    \"\"\"\n",
    "    choose sample input and plot the attention associated to the argmax probability vocab at each input position \n",
    "    (memory_seq versus each input element), call plot_seq_attention as subfunction\n",
    "    \"\"\"\n",
    "    b = sample_input_index\n",
    "    sample_input = seqs_x_train[b:b+1]\n",
    "    print(\"sample_input: \",sample_input)\n",
    "    sample_mem_seq_x = mem_seqs_x_train[b:b+1]\n",
    "    sample_mem_seq_y = mem_seqs_y_train[b:b+1]\n",
    "\n",
    "    pred = memory_model([sample_input, sample_mem_seq_x, sample_mem_seq_y])\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    print(f'prediction: {pred[0]}')\n",
    "    print(f'y: {seqs_y_train[b]}')\n",
    "    print(pred[0] == seqs_y_train[b])\n",
    "    attn_ = memory_model.hierarchical_mem_attention.last_per_seq_attn_mat[0]\n",
    "\n",
    "    fig = plot_per_seq_attention(attn_, sample_mem_seq_x[0], sample_input[0])\n",
    "    attn_ = memory_model.hierarchical_mem_attention.last_mem_seq_attn_mat[0]\n",
    "    plot_seq_attention(attn_, sample_mem_seq_x[0], sample_input[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot function for kernel model 1\n",
    "from utils_kernel_transformer import plot_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot function for kernel cross attn value = y model \n",
    "\n",
    "from utils_kernel_transformer import plot_seq_attention\n",
    "\n",
    "def plot_attention_cross_y(sample_input_index, seqs_x_train, seqs_y_train, mem_seqs_x_train, mem_seqs_y_train, memory_model):\n",
    "    \"\"\"\n",
    "    choose sample input and plot the attention associated to the argmax probability vocab at each input position \n",
    "    (memory_seq versus each input element), call plot_seq_attention as subfunction\n",
    "    \"\"\"\n",
    "    b = sample_input_index\n",
    "    sample_input = seqs_x_train[b:b+1]\n",
    "    print(\"sample_input: \",sample_input)\n",
    "    sample_mem_seq_x = mem_seqs_x_train[b:b+1]\n",
    "    sample_mem_seq_y = mem_seqs_y_train[b:b+1]\n",
    "\n",
    "    pred = memory_model([sample_input, sample_mem_seq_x, sample_mem_seq_y])\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    print(f'prediction: {pred[0]}')\n",
    "    print(f'y: {seqs_y_train[b]}')\n",
    "    print(pred[0] == seqs_y_train[b])\n",
    "    attn_ = memory_model.alpha_it[0]\n",
    "    #print(attn_output_seq.shape)\n",
    "    plot_seq_attention(attn_, sample_mem_seq_x[0], sample_input[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention on training set\n",
    "for idx in [10,100,200, 300, 500,700,800,900,999]:\n",
    "    plot_attention(idx, seqs_x_train, seqs_y_train, mem_seqs_x_train, mem_seqs_y_train, memory_model1)\n",
    "    plot_attention_hierarchical(idx, seqs_x_train, seqs_y_train, mem_seqs_x_train, mem_seqs_y_train, memory_model2)\n",
    "    plot_attention_cross_y(idx, seqs_x_train, seqs_y_train, mem_seqs_x_train, mem_seqs_y_train, memory_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention on testing set\n",
    "for idx in [0, 1, 10,100,200, 300, 350, 399]:\n",
    "    plot_attention(idx, seqs_x_test, seqs_y_test, mem_seqs_x_train, mem_seqs_y_train, memory_model1)\n",
    "    plot_attention_hierarchical(idx, seqs_x_test, seqs_y_test, mem_seqs_x_train, mem_seqs_y_train, memory_model2)\n",
    "    plot_attention_cross_y(idx, seqs_x_test, seqs_y_test, mem_seqs_x_train, mem_seqs_y_train, memory_model3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness: change memory at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_seqs_x, mem_seqs_y = sample_mem_dataset(mem_seq_len, mem_size)\n",
    "\n",
    "\n",
    "# NOTE: same memory buffer for all inputs\n",
    "mem_seqs_x_test = np.repeat(np.expand_dims(mem_seqs_x, axis=0), axis=0, repeats=n_test)\n",
    "mem_seqs_y_test = np.repeat(np.expand_dims(mem_seqs_y, axis=0), axis=0, repeats=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel model 1\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_test, mem_seqs_x_test), seqs_y_test)).batch(64)\n",
    "memory_model1.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model1.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical attn model 2\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((seqs_x_test, mem_seqs_x_test, mem_seqs_y_test), seqs_y_test)).batch(64)\n",
    "memory_model2.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model2.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel model 3\n",
    "memory_model3.evaluate(test_ds)\n",
    "\n",
    "# plot accuracy as a function of position in sequence\n",
    "preds = memory_model3.predict(train_ds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "acc_by_pos = np.mean(preds == seqs_y_train, axis=0)\n",
    "plt.plot(acc_by_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
